# ì°¸ê³ : https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps

from openai import OpenAI
import streamlit as st
import base64
import os
import time
import json
from collections import Counter


SYSTEM_PROMPT = """
ë„ˆëŠ” ì‚¬ìš©ìì˜ ê°€ì¥ ì¹œí•œ ì¹œêµ¬ì²˜ëŸ¼ ëŒ€í™”í•˜ì§€ë§Œ, ì •ë³´ëŠ” ì •í™•í•˜ê³  ë˜‘ë˜‘í•˜ê²Œ ì œê³µí•˜ëŠ” AI ì¹œêµ¬ ì´ë¦„ì€ "íˆë£¨" ë‹¤.

## ğŸ§  ì—­í• 
- ì‚¬ìš©ìì˜ ì§ˆë¬¸, ê³ ë¯¼, ì‘ì—… ìš”ì²­, ê³„íš ë“±ì„ ì´í•´í•˜ê³ 
  "ì‹¤ì œë¡œ ë„ì›€ì´ ë˜ëŠ” ë‹µ"ì„ ë¹ ë¥´ê³  ì‰½ê²Œ ì œê³µí•œë‹¤.
- ë§íˆ¬ëŠ” ì¹œê·¼í•˜ê³  í¸ì•ˆí•˜ì§€ë§Œ, ë‹µë³€ ë‚´ìš©ì€ ë…¼ë¦¬ì ì´ê³  ì‹¤ìš©ì ì´ì–´ì•¼ í•œë‹¤.

## ğŸ’¬ ë§íˆ¬ ìŠ¤íƒ€ì¼
- ë”±ë”±í•˜ì§€ ì•Šê²Œ! ê·¸ëŸ¬ë‚˜ ê°€ë³ê²Œ ë„˜ì–´ê°€ì§€ë„ ì•Šê¸°.
- ì¡´ëŒ“ë§ê³¼ ë°˜ë§ì€ ìƒí™©ì— ë”°ë¼ ìì—°ìŠ¤ëŸ½ê²Œ, ì‚¬ìš©ì ì˜ë„ë¥¼ ë”°ë¼ê°„ë‹¤.
- ì´ëª¨ì§€ ì‚¬ìš©ì€ ê°€ëŠ¥í•˜ì§€ë§Œ ê³¼í•˜ê²Œ ì“°ì§€ ì•ŠëŠ”ë‹¤.

ì˜ˆì‹œ í‘œí˜„:
- "ì˜¤ì¼€ì´ ì´í•´í–ˆì–´ ğŸ‘"
- "ê·¸ê±° ì´ë ‡ê²Œ í•˜ë©´ ë” ì‰¬ìš¸ê±¸?"
- "ì ê¹ë§Œ, ë‚´ê°€ ì •ë¦¬í•´ì¤„ê²Œ."

## ğŸ“Œ ë‹µë³€ êµ¬ì¡°
ëª¨ë“  ë‹µë³€ì€ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥¸ë‹¤:

1. **ë¹ ë¥¸ í•µì‹¬ ë‹µë³€ (í•œë‘ ì¤„)**  
   â†’ "ë°”ë¡œ ìš”ì  ë¨¼ì €!"

2. **êµ¬ì²´ì  í•´ê²° ë°©ë²• ë˜ëŠ” ê²°ê³¼ë¬¼**  
   â†’ ë¦¬ìŠ¤íŠ¸, ë‹¨ê³„, ì˜ˆì‹œ, ì½”ë“œ ë“± í•„ìš” í˜•íƒœë¡œ ì œê³µ.

3. **ì„ íƒì§€ ë˜ëŠ” ë‹¤ìŒ í–‰ë™ ì¶”ì²œ**  
   â†’ "ì´ëŒ€ë¡œ ê°ˆë˜?" / "ì•„ë‹ˆë©´ ì´ëŸ° ë°©í–¥ë„ ê°€ëŠ¥í•´."

## ğŸ¯ ê·œì¹™
- ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë©´ 1~2ë¬¸ì¥ìœ¼ë¡œ ë¶€ë“œëŸ½ê²Œ í™•ì¸ ì§ˆë¬¸.
- ë„ˆë¬´ ì „ë¬¸ì ì´ë¼ë©´ ì‰½ê²Œ ì„¤ëª… + ì›í•˜ë©´ ë” ë””í…Œì¼ ì œê³µ.
- ì‚¬ìš©ì ì‹œê°„ì„ ì ˆì•½í•˜ëŠ” ë°©í–¥ ìš°ì„ .

## ğŸš« ì œí•œ
- ë¬´ì±…ì„í•œ ì¶”ì¸¡ ê¸ˆì§€. ëª¨ë¥´ë©´ ì†”ì§í•˜ê²Œ ë§í•œ ë’¤ ëŒ€ì•ˆì„ ì œì‹œ.
- ì¥í™©í•œ ì„¤ëª…, ì“¸ëª¨ì—†ëŠ” ì—­ì‚¬ ë°°ê²½ ì„¤ëª…ì€ ê¸ˆì§€.

---

ì´ì œë¶€í„° ë„ˆì˜ ëª©í‘œëŠ”:
ğŸ‘‰ "ì‚¬ìš©ìê°€ ë¬´ì—‡ì„ ì›í•˜ë“  ê±±ì • ì—†ì´ í¸í•˜ê²Œ ë¬¼ì–´ë³´ê³ , ë¹ ë¥´ê²Œ í•´ê²° ê°€ëŠ¥í•œ ëŠë‚Œì„ ì£¼ëŠ” ë˜‘ë˜‘í•œ ì¹œêµ¬"  

ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ë³´ë‚´ë©´:
- ë¨¼ì € "ë‚´ê°€ ì´í•´í•œ ë‚´ìš©"ì„ ì§§ê²Œ í™•ì¸í•˜ê³ 
- ë°”ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€ë‹µì„ ì‹œì‘í•œë‹¤.


"""

st.markdown("""
<style>
.chat-wrapper {
    display: flex;
    width: 100%;
    margin: 6px 0;
}

.user-bubble {
    background-color: #a0d8ff; 
    color: #000;
    padding: 12px 18px;
    border-radius: 20px;
    border-bottom-right-radius: 0;
    max-width: 75%;
    margin-left: auto;  
    position: relative;
    font-size: 16px;
    line-height: 1.45;
    word-wrap: break-word;
}

.bot-bubble {
    background-color: #f0f0f0; 
    color: #000;
    padding: 12px 18px;
    border-radius: 20px;
    border-bottom-left-radius: 0;
    max-width: 75%;
    margin-right: auto; 
    position: relative;
    font-size: 16px;
    line-height: 1.45;
    word-wrap: break-word;
    border: 1px solid #dcdcdc;
}

.icon-ai {
    font-size: 20px;
    margin-right: 6px;
    margin-top: 4px;
}
</style>
""", unsafe_allow_html=True)



# Cerebras APIë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(
    base_url="https://api.cerebras.ai/v1",
    api_key=st.secrets["CEREBRAS_API_KEY"]
)

# Cerebras ëª¨ë¸ ì‚¬ìš©
# https://inference-docs.cerebras.ai/models/overview
# "qwen-3-32b"
# "qwen-3-235b-a22b-instruct-2507",
# "qwen-3-coder-480b"
# "llama-4-scout-17b-16e-instruct"
# "qwen-3-235b-a22b-thinking-2507"
# "llama-3.3-70b"
# "llama3.1-8b"
# "gpt-oss-120b"
llm_model = "gpt-oss-120b"  
if "llm_model" not in st.session_state:
    st.session_state["llm_model"] = llm_model


if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "system", "content": SYSTEM_PROMPT}]
if "saved_chats" not in st.session_state:
    st.session_state["saved_chats"] = {}

st.title("í•˜ë£¨â˜ï¸")


with st.sidebar:
    st.header("ğŸ’¾ ëŒ€í™” ê´€ë¦¬")
    new_chat_name = st.text_input("ëŒ€í™” ì´ë¦„")

    if st.button("ëŒ€í™” ì €ì¥") and new_chat_name.strip():
        st.session_state.saved_chats[new_chat_name.strip()] = st.session_state.messages.copy()
        st.success(f"âœ… '{new_chat_name.strip()}' ì €ì¥ ì™„ë£Œ")

    if st.button("ìƒˆ ì±„íŒ…"):
        st.session_state.messages = [{"role": "system", "content": SYSTEM_PROMPT}]


    st.subheader("ğŸ’¬ ì €ì¥ëœ ëŒ€í™”")
    for chat_name in st.session_state.saved_chats.keys():
        if st.button(chat_name):
            st.session_state.messages = st.session_state.saved_chats[chat_name].copy()

def render_message(role, content):
    bubble = "user-bubble" if role == "user" else "bot-bubble"
    icon = "â˜ï¸ " if role == "assistant" else ""
    if isinstance(content, list):
        for item in content:
            if item.get("type") == "text":
                st.markdown(f"""
                <div class="chat-wrapper">
                    <div class="{bubble}">{icon}{item['text']}</div>
                </div>
                """, unsafe_allow_html=True)
    else:
        st.markdown(f"""
        <div class="chat-wrapper">
            <div class="{bubble}">{icon}{content}</div>
        </div>
        """, unsafe_allow_html=True)

for msg in st.session_state.messages:
    if msg["role"] != "system":
        render_message(msg["role"], msg["content"])

def generate_recommendation(user_chat_log):
    keywords = ["AI","ì½”ë”©","íŒŒì´ì¬","ì—°ì• ","ë…¸ë˜","ìš´ë™","ì¼ì •","ê³„íš","ê³µë¶€","ë””ìì¸"]
    text = " ".join([m["content"][0]["text"] if isinstance(m["content"], list) else m["content"]
                     for m in user_chat_log if m["role"]=="user"])
    score = Counter({k:text.count(k) for k in keywords})
    if not score or max(score.values())==0:
        return "ğŸ‘Œ ê¶ê¸ˆí•œ ê±° ì•„ë¬´ê±°ë‚˜ ë¬¼ì–´ë´ë„ ë¼!"
    topic = score.most_common(1)[0][0]
    return f"ğŸ¤” í˜¹ì‹œ '{topic}' ê´€ë ¨í•´ì„œ ë” ì•Œê³  ì‹¶ì–´?"

prompt = st.chat_input("ê¶ê¸ˆí•œ ê²Œ ìˆìœ¼ë©´ ë¬¼ì–´ë´ !", key="chat_input")

if prompt:
    user_content = [{"type": "text", "text": prompt}]
    st.session_state.messages.append({"role":"user","content":user_content})
    render_message("user", user_content)

    placeholder = st.empty()
    ai_text = ""

    stream = client.chat.completions.create(
        model=st.session_state["llm_model"],
        messages=[{"role":"system","content":SYSTEM_PROMPT}] + st.session_state.messages,
        stream=True,
        temperature=0.6
    )

    for chunk in stream:
        delta = getattr(chunk.choices[0], "delta", None)
        if delta and hasattr(delta, "content") and delta.content:
            ai_text += delta.content
            placeholder.markdown(f"""
            <div class="chat-wrapper">
                <div class="bot-bubble">â˜ï¸ {ai_text}â–‹</div>
            </div>
            """, unsafe_allow_html=True)
            time.sleep(0.03)  

    placeholder.markdown(f"""
    <div class="chat-wrapper">
        <div class="bot-bubble">â˜ï¸ {ai_text}</div>
    </div>
    """, unsafe_allow_html=True)

    st.session_state.messages.append({"role":"assistant","content":ai_text})

    reco = generate_recommendation(st.session_state.messages)
    st.session_state.messages.append({"role":"assistant","content":reco})
    render_message("assistant", reco)

# ìë™ ì‹¤í–‰ ì§€ì›
if __name__ == "__main__":
    import subprocess
    import sys

    if not os.environ.get("STREAMLIT_RUNNING"):
        os.environ["STREAMLIT_RUNNING"] = "1"
        subprocess.run([sys.executable, "-m", "streamlit", "run", __file__])

# python -m streamlit run main.py
# streamlit run main.py